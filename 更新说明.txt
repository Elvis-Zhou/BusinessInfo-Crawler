BusinessInfo-Crawler

v1.0 更新说明
中山采购 v1.0
这是一个为某个公司所做的外包项目，是我一个人承担的。
公司需要的是公司数据，而我是锻炼技术。

这是一个集合了 网络爬虫，指定地点与关键词，爬取针对的黄页上的所有公司信息。
以及能对Google的数据进行爬取。
还有涉及到网页文本摘要。
采用了多线程技术，BeautifulSoup，正则，浏览器伪装，cookie伪装等方式的爬虫
后来会再进行框架的开发。
其他的慢慢补充。。。

项目文件打包

请确保Python 的环境变量配置好了。
参考http://www.cnblogs.com/babykick/archive/2011/03/25/1995994.html 的第1、2步骤即可

然后双击运行 第一次使用初始化环境.bat 
文件对应什么模块请看：说明文件.txt
运行脚本方式：
右键 py文件， 选择 Edit With IDLE
然后 按F5 运行


findUrls.py 是模块1   寻找GOOGLE 特定URL
ContactFinder.py 是模块2  寻找页面内的公司信息，EMAIL,TEL,ADDRESS等等。。 还有文本提取
yellowPageDataming.py 是模块3 某个黄页的信息提取
FilterRegular.txt是过滤规则  过滤掉不要的页面
database.db是数据库文件 自己设计用来存储这些信息的数据库


v1.1 更新说明
1、	模块1中的链接提取，按照要求改成提取根目录了。
2、	模块1 参数可以自定义了：可以输入 
	查询的关键词
	要获取的最大页数，
	休息时间
3、	模块2 数据交错的问题，采用了锁来防止这种情况的发生。
4、	模块2 添加了 Email过滤和 网址过滤的功能。不会产生重复。
5、	解决了某些时候爬不到黄页邮箱的问题：
6、	黄页输入参数的自定义功能：
可以输入 
	查询的关键词
	要获取的最大页数，
	线程数
	是否查询Location.txt中的地区
7、	把完整资料存在rawinformation 里面的功能
把那个页面内的公司相关信息都放入rawInformation中
8、	根据地区来搜索功能 
a.	我输入关键字，
b.	支持多地区 导入比如说有个文档每一行就是一个地区，如果没有文档就不搜索地区
c.	我可以设置最多多少页
这些功能全部都实现了
9、	邮箱地址过滤功能：
邮箱储存结果过滤与去重复功能，已实现。
规则在 FilterMails.txt 中可以自定义。
至此，智利黄页需要的所有新添加的功能都实现了。

v1.2 更新说明
1、url抓取那边
先过滤，去重复，然后再写入urls里面的数据库。
2、 电邮，地址抓取，
找contact us，about us，support这样的里面去抓取 电邮，电话，地址。。。
现在是正则表达式 ，
找寻 contact 
aboutus
support
有关的所有链接（只要含这样的关键词）就算。
优先级是：
contact>aboutus>support
3、多线程方式修改
4、增加 文本标签密度算法 提取网页正文
5、增加更多的 屏幕输出提示

v1.3 更新说明
1、Yell网站爬虫已经实现。
2、输入参数自定义功能
可以输入 
	查询的关键词
	要获取的最大页数，
	线程数（在Yell中强烈建议只使用单线程，因为yell的反爬虫。）
	是否查询Location.txt中的地区
3、Yell功能和智利黄页一样，过滤功能，去重复功能都有。
主要的区别是：
Yell中爬取到的数据是没有邮箱的，所以爬虫会自动去寻找这个公司的主页，然后链接到之前的模块2，去查询联系方式，找到邮箱并返回，这样就能获得所有的信息：公司名，地址，网址，邮箱，电话等等。
至此，Yell的全部功能都已经实现了。


v1.4 更新说明
1、192网站的所有功能完成。
2、昨天对谷歌采用多线程，结果被屏蔽了一天。今天修改了多线程策略，解决了这个问题。
3、192结果解析，必须链接到模块1，模块2，很麻烦。目前采用20线程跑（每页结果数是20），20线程是最佳的结果。
4、修改与增加了模块2的寻找联系页面的策略。
5、增加了网址过滤规则。
6、关键词按照要求修改为 文件读入keywords.txt 
7、模块位置重新分割，按照文件夹放置。

v1.5 更新说明
项目重构的架构

1. 设计新的数据库支持这种模式的。
Information表
10个字段  （TEXT是Sqlite的字符串文本类型）
Keyword  		  类型  TEXT
Url				  类型  TEXT（还是用来存储联系方式页面的Url）
Name	  		  类型  TEXT
Homepage   	  类型  TEXT (这个新增的，用来存储公司主页的，以免有的公司联系方式页面不存在时，Url为空)
Email	  类型  TEXT
Country  类型  TEXT
Address	  类型  TEXT
Tel		  类型  TEXT
RawInformation 类型TEXT
SearchTimes 类型TEXT （0表示从未被搜索引擎检索过，每检索一次，此值+1，方便以后更新数据的时候有标识）

Url表：
5个字段  （TEXT是Sqlite的字符串文本类型）
Keyword  类型  TEXT
Title 	  类型  TEXT
Url 	 	  类型  TEXT
Country  类型 TEXT
Dealed    类型  TEXT（0为未处理，1为已经处理过了）

2.黄页信息提取模块
   2.1 重写共用母类
	逻辑调度引擎
	爬虫伪装cookie以及User-agent
	网页信息下载器
	任务队列
	Url入口规则生成器（包含主入口Url以及每个页面Url入口规则）
	列表页面解析器（若列表中已经包含了公司信息，则包含公司页面解析）
	公司页面解析器
	多线程（保证原子性，避免数据交错）
	数据整合与生成
	数据库读写

   2.2 每个具体黄页或者其他信息端，继承母类别，重载方法
黄页页面解析主要分成3种：
1.	在搜索结果页面已经包含了所有详细信息。比如Yell
2.	在搜索结果页面，比如进入黄页的公司页面，才能开始抓取信息。比如第一次做的那个网站Amarillas
3.	在搜索结果页面几乎没有什么联系信息，大部分需要google。比如192

其中，继承后，主要需要重载：
	Url入口规则生成器（包含主入口Url以及每个页面Url入口规则）
	列表页面解析器
	公司页面解析器
	第2种情况，需要特别处理：先把公司页面存入数据库，然后全部存完以后开始解析。解析过程中需要及时标识哪些是处理过的。

3.搜索引擎补全信息模块
3.1.取待处理队列:
在Information表中，
SELECT * FROM Information
WHERE SearchTimes=’0’ AND Email=’’

当搜索次数为0，并且email为空的时候，构建一个待搜索队列。

3.2寻找公司主页模块
重构原来的模块1
主要目标：
	取待处理队列
	通过公司的关键词
	准确获得公司主页。

3.3 寻找公司联系信息模块
重构原来的模块2
主要目标：
	获得公司主页后，找寻这个主页的联系信息。
	此时需要过滤掉无用的非公司网站，比如wikipedia.org等

3.4 数据更新模块：
整合并重构以前的几个信息过滤、去重复等模块。
主要目标：
	获得搜索引擎返回的数据，比对此数据与数据库中数据的关系，比如有多个email，则增量更新。
	去重复
	过滤（邮箱过滤和网址过滤）
	补全缺失的信息

4. 文件I/O模块
	输入关键词：Keywords.txt
	输入地区：Location.txt
	输入邮箱过滤规则：FilterMails.txt
	输入网站过滤规则：FilterRegular.txt


整个框架的调度图可以参考下图：

关于参考的框架是Python一个优秀的开源爬虫，Scrapy 
http://www.scrapy.org/


v2.0 更新说明
主要是：重构并进行测试，保证重构后的稳定性。

1、统一数据输入：
输入的文档关键字是和之前说的一样会事类别，和关键字，然后另外一个文档时地址
所以以后统一所有的黄页都是这个格式。
2、多email的处理
3、存在同名公司的处理
4、更新屏幕输出提示，全改成英文
5、修复可能会出现的bug，database is locked
6、google的查询独立出来了，可以看成类似一个黄页。

v2.1 更新说明
1.、解决了信息补全模块中存在的bug。
2、处理DATABASE IS LOCKED 数据库同时在多个程序访问时的问题，避免该情况的发生。
3、多email的处理方式根据用户需求有变化：现在在联系方式页面中，匹配到的所有email，会每行一个存储。
用户希望的数据时1条一行，这样方便邮件群发软件使用。
4、删除部分中文print，统一英文输出显示。

v2.2 更新说明
1、信息补全的策略：
 调用公司名字，自动输入google里面
寻找到公司链接
进入网站，寻找contact page
找到以后，更新数据库中的信息
2、针对不同国家采用不同的搜索引擎
例如这样：
if country==uk then  google.co.uk/?=led&gl=uk
if country==cl then google.com.cl/?=led&gl=cl
3、解决遗漏categroy 和keyword的bug
4、对于把google当成黄页的时候，就也把结果理解成 公司名字，国家，主页。
然后运行那个补全就能自动补全了，这样逻辑上面google黄页也和yell，192一样的了，不需要单独对待了。
