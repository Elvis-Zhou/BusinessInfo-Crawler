BusinessInfo-Crawler

v1.0 更新说明
中山采购 v1.0
这是一个为某个公司所做的外包项目，是我一个人承担的。
公司需要的是公司数据，而我是锻炼技术。

这是一个集合了 网络爬虫，指定地点与关键词，爬取针对的黄页上的所有公司信息。
以及能对Google的数据进行爬取。
还有涉及到网页文本摘要。
采用了多线程技术，BeautifulSoup，正则，浏览器伪装，cookie伪装等方式的爬虫
后来会再进行框架的开发。
其他的慢慢补充。。。

项目文件打包

请确保Python 的环境变量配置好了。
参考http://www.cnblogs.com/babykick/archive/2011/03/25/1995994.html 的第1、2步骤即可

然后双击运行 第一次使用初始化环境.bat 
文件对应什么模块请看：说明文件.txt
运行脚本方式：
右键 py文件， 选择 Edit With IDLE
然后 按F5 运行


findUrls.py 是模块1   寻找GOOGLE 特定URL
ContactFinder.py 是模块2  寻找页面内的公司信息，EMAIL,TEL,ADDRESS等等。。 还有文本提取
yellowPageDataming.py 是模块3 某个黄页的信息提取
FilterRegular.txt是过滤规则  过滤掉不要的页面
database.db是数据库文件 自己设计用来存储这些信息的数据库


v1.1 更新说明
1、	模块1中的链接提取，按照要求改成提取根目录了。
2、	模块1 参数可以自定义了：可以输入 
	查询的关键词
	要获取的最大页数，
	休息时间
3、	模块2 数据交错的问题，采用了锁来防止这种情况的发生。
4、	模块2 添加了 Email过滤和 网址过滤的功能。不会产生重复。
5、	解决了某些时候爬不到黄页邮箱的问题：
6、	黄页输入参数的自定义功能：
可以输入 
	查询的关键词
	要获取的最大页数，
	线程数
	是否查询Location.txt中的地区
7、	把完整资料存在rawinformation 里面的功能
把那个页面内的公司相关信息都放入rawInformation中
8、	根据地区来搜索功能 
a.	我输入关键字，
b.	支持多地区 导入比如说有个文档每一行就是一个地区，如果没有文档就不搜索地区
c.	我可以设置最多多少页
这些功能全部都实现了
9、	邮箱地址过滤功能：
邮箱储存结果过滤与去重复功能，已实现。
规则在 FilterMails.txt 中可以自定义。
至此，智利黄页需要的所有新添加的功能都实现了。

v1.2 更新说明
1、url抓取那边
先过滤，去重复，然后再写入urls里面的数据库。
2、 电邮，地址抓取，
找contact us，about us，support这样的里面去抓取 电邮，电话，地址。。。
现在是正则表达式 ，
找寻 contact 
aboutus
support
有关的所有链接（只要含这样的关键词）就算。
优先级是：
contact>aboutus>support
3、多线程方式修改
4、增加 文本标签密度算法 提取网页正文
5、增加更多的 屏幕输出提示

v1.3 更新说明
1、Yell网站爬虫已经实现。
2、输入参数自定义功能
可以输入 
	查询的关键词
	要获取的最大页数，
	线程数（在Yell中强烈建议只使用单线程，因为yell的反爬虫。）
	是否查询Location.txt中的地区
3、Yell功能和智利黄页一样，过滤功能，去重复功能都有。
主要的区别是：
Yell中爬取到的数据是没有邮箱的，所以爬虫会自动去寻找这个公司的主页，然后链接到之前的模块2，去查询联系方式，找到邮箱并返回，这样就能获得所有的信息：公司名，地址，网址，邮箱，电话等等。
至此，Yell的全部功能都已经实现了。


v1.4 更新说明
1、192网站的所有功能完成。
2、昨天对谷歌采用多线程，结果被屏蔽了一天。今天修改了多线程策略，解决了这个问题。
3、192结果解析，必须链接到模块1，模块2，很麻烦。目前采用20线程跑（每页结果数是20），20线程是最佳的结果。
4、修改与增加了模块2的寻找联系页面的策略。
5、增加了网址过滤规则。
6、关键词按照要求修改为 文件读入keywords.txt 
7、模块位置重新分割，按照文件夹放置。
